#####################################################################
# Source code of the ensemble error detection method
# Authors: Mohamed Abdelaal
# Date: June 2022
# Software AG
# All Rights Reserved
#####################################################################

import os
import sys
import numpy as np
import pandas as pd
from sklearn import preprocessing
from aug2clean.dataset import dataset
from aug2clean.setup.detectors.detect import detect
from aug2clean.setup.utils import create_detections_path
from aug2clean.setup.data_augmentation.utils import MultiColumnLabelEncoder
from aug2clean.setup.data_augmentation.variational_autoencoder import run_vae
from aug2clean.setup.detectors.detect_method import DetectMethod, EXP_PATH, DATA_PATH


def ensemble_detector(dirty_path,
                      clean_path,
                      detections_path,
                      dataset_path,
                      dataset_name,
                      detectors_list=[],
                      verbose=True):
    """
    Detect errors using an adaptive ensemble method
    """

    if os.path.exists(detections_path):
        print("Detections already exist") if verbose else lambda *a: None
        # Load the detections if they already exist
        reader = pd.read_csv(detections_path, names=['i', 'j', 'dummy'])
        detections = reader.groupby(['i', 'j'])['dummy'].apply(list).to_dict()
        return detections

    # Create a list of all available detectors
    available_detectors = list(DetectMethod)

    # Use all available base detectors if no specific detectors are selected
    involved_detectors = available_detectors if not detectors_list else detectors_list
    if verbose:
        print("[INFO] The involved base detectors are: {}".format(involved_detectors))

    # Initialize a detection dictionary
    detection_dicts = []

    for detector in involved_detectors:
        # Check if detections already exist and generate
        if detector.__str__() not in ['min_k', 'aug2clean']:
            path = create_detections_path(EXP_PATH, dataset_name, detector.__str__(), create_new_dirs=True)
            # Check whether the detections file exist
            if os.path.exists(path):
                try:
                    if verbose:
                        print("[INFO] Loading detections generated by the {} detector ..".format(detector.__str__()))
                    reader = pd.read_csv(path, names=['i', 'j', 'dummy'])
                    detection_dicts.append(reader.groupby(['i', 'j'])['dummy'].apply(list).to_dict())
                except:
                    print("[ERROR] Failed to load the detections generated by {}".format(detector.__str__()))
                    continue
            else:
                if verbose:
                    print("[INFO] Executing the {} detector ..".format(detector.__str__()))
                try:
                    # Detect the errors
                    detection_dicts.append(detect(clean_path, dirty_path, path, dataset_path, dataset_name,
                                                  detect_method=detector))
                except:
                    print("[ERROR] Failed to execute the {} detector".format(detector.__str__()))
                    continue
        else:
            continue

    # For each detected cell, count the number of times it was detected over all detection_dicts
    cells_counter = {}
    for i, detections in enumerate(detection_dicts):
        if type(detections) == dict:
            for cell in detections.keys():
                if cell not in cells_counter:
                    cells_counter[cell] = 0.0
                cells_counter[cell] += 1.0
        else:
            continue

    # for each detected error get percentage of detectors that detected the error
    for cell in cells_counter:
        cells_counter[cell] /= len(detection_dicts)

    return cells_counter


def adaptive_sampler(dirty_df,
                     cells_counter,
                     initial_threshold,
                     missing_classes=[],
                     labels='',
                     problem_type='',
                     exclusion_problem=False,
                     verbose=True):
    """Combine the detections of base detectors based on the threshold k. It adjusts the value of the threshold to
    overcome possible data exclusion problems."""

    # Initialize the variables
    new_threshold = 0
    label_loc = 0

    # update the threshold if the data exclusion problem occurred
    if exclusion_problem:
        # Get the index of the label column which will be used to update the threshold only for the missing classes
        label_loc = dirty_df.columns.get_loc(labels)
        # Increase the threshold to overcome the data exclusion problem
        new_threshold = initial_threshold + 0.1
        if verbose:
            print("[INFO] Increasing the threshold from {} to {}".format(initial_threshold, new_threshold))

    # fill detection_dictionary with detections that have been detected
    # by a minimum of threshold-percent of the detectors
    detection_dictionary = {}
    for cell in cells_counter:

        # Use the updated threshold only if the cell exists in the missing classes, otherwise use the initial threshold
        if exclusion_problem and problem_type == 'Attribute level':
            threshold = new_threshold
        elif exclusion_problem and problem_type == 'Class level':
            threshold = new_threshold if dirty_df.iat[cell[0],label_loc] in missing_classes else initial_threshold
        else:
            threshold = initial_threshold

        if cells_counter[cell] >= threshold:
            detection_dictionary[cell] = "JUST A DUMMY VALUE"

    # Sample a clean fraction
    clean_fraction = dirty_df.drop([cell_index[0] for cell_index in detection_dictionary.keys()])

    return clean_fraction, new_threshold


def aug2clean(dirty_path,
              clean_path,
              detections_path,
              dataset_path,
              dataset_name,
              detectors_list=[],
              threshold=0.2,
              nb_samples=6000,
              prevent_data_exclusion=True,
              verbose=True):
    """
    Detect errors using the adaptive min-k, before generating additional data based on the clean fraction. To this
    end, we divide the dirty data into erronous rows and clean rows, before using the clean rows to generate the
    clean fraction as a dataframe. Such a clean fraction is used to train a variational autoencoder to generate
    additional data
    TODO The adaptive ensemble method relaxes the Ml-based error detection. Initially, we detect error using only an
    ML-based method (ED2), but other detectors are considered if one of the data exclusion problems occurred.
    """

    # Identify the list of classes in the dirty data set
    # Define a data object to get the labels
    data_obj = dataset.Dataset(dataset_name)
    labels = data_obj.cfg.labels
    ml_task = data_obj.cfg.ml_task

    # Load the dirty dataset
    dirty_df = pd.read_csv(dirty_path, header="infer", encoding="utf-8", low_memory=False)

    # Fill the NAN cells
    dirty_df.fillna(method='ffill', inplace=True)

    # Encode the categorical columns
    object_columns = dirty_df.iloc[:, :].select_dtypes(include=['object']).columns
    if object_columns.any():
        try:
            if verbose:
                print("[INFO] Categorical columns: {}".format(object_columns))
            encoder = MultiColumnLabelEncoder(columns=object_columns)
            encoder.fit_transform(dirty_df)
        except:
            print("[ERROR] Failed to encode the categorical columns of the dirty data set")

    if ml_task == 'binary_classification':
        le_labels = preprocessing.LabelEncoder()
        dirty_df[labels] = le_labels.fit_transform(dirty_df[labels])

    # Detect errors in the dirty data set
    cells_counter = ensemble_detector(dirty_path, clean_path, detections_path, dataset_path, dataset_name,
                                      detectors_list=detectors_list, verbose=verbose)

    # Extract a clean fraction
    clean_fraction, _ = adaptive_sampler(dirty_df, cells_counter, initial_threshold=threshold,
                                         verbose=verbose, labels=labels)

    if prevent_data_exclusion:
        # Get the list of classes in the extracted clean fraction and in the dirty data set
        classes = np.unique(dirty_df[labels])
        clean_fraction_classes = np.unique(clean_fraction[labels])

        # Check for the relevant data exclusion: class-level and attribute-level
        while clean_fraction.empty or set(clean_fraction_classes) != set(classes):
            # Get the missing classes
            missing_classes = list(set(classes) - set(clean_fraction_classes))
            # Identify the type of the problem
            problem_type = 'Attribute level' if clean_fraction.empty else 'Class level'
            if verbose:
                print("[INFO] {} data exclusion has occurred!".format(problem_type))
            # Adapt the threshold and re-estimate the clean fraction
            clean_fraction, new_threshold = adaptive_sampler(dirty_df, cells_counter,
                                                             initial_threshold=threshold,
                                                             missing_classes=missing_classes,
                                                             labels=labels,
                                                             problem_type=problem_type,
                                                             exclusion_problem=True, verbose=verbose)
            # Update the threshold
            threshold = new_threshold
            # Update the list of classes in the clean fraction
            clean_fraction_classes = np.unique(clean_fraction[labels])

    # Augment the clean fraction
    generated_df = run_vae(clean_fraction, dataset_name, epochs=20, nb_samples=nb_samples)

    return generated_df, dirty_df


if __name__ == '__main__':
    # Get the data path
    dataset_name = 'nasa'
    method = DetectMethod.AUG2CLEAN

    dataset_path = os.path.abspath(os.path.join(DATA_PATH, dataset_name))
    # Retrieve the dirty and clean data
    clean_path = os.path.abspath(os.path.join(DATA_PATH, dataset_name, 'clean.csv'))
    dirty_path = os.path.abspath(os.path.join(DATA_PATH, dataset_name, 'dirty.csv'))

    # Create a path to the detections.csv file
    detections_path = create_detections_path(EXP_PATH, dataset_name, method.__str__())

    aug2clean(dirty_path, clean_path, detections_path, dataset_path, dataset_name, detectors_list=[],
              threshold=0.2, verbose=True)
